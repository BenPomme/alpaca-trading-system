#!/usr/bin/env python3
"""
ML System Audit - Comprehensive Test of ML Integration
Tests all aspects of ML data collection, parameter optimization, and learning
"""

import sys
import os
from datetime import datetime, timedelta

def test_ml_data_collection():
    """Test ML data collection across all modules"""
    print("üß™ TESTING ML DATA COLLECTION")
    print("="*50)
    
    try:
        from modular.base_module import TradingModule, ModuleConfig, TradeOpportunity, TradeAction
        from modular.firebase_interface import ModularFirebaseInterface
        from modular.ml_data_helpers import MLDataCollector, ParameterEffectivenessTracker
        from modular.crypto_module import CryptoModule
        
        # Test ML data collector
        ml_collector = MLDataCollector("test_module")
        
        # Test entry parameters creation
        entry_params = ml_collector.create_entry_parameters(
            confidence_threshold_used=0.65,
            position_size_multiplier=1.5,
            regime_confidence=0.75,
            technical_confidence=0.82,
            pattern_confidence=0.68,
            ml_strategy_selection=True,
            leverage_applied=2.0
        )
        
        print("‚úÖ Entry parameters created successfully")
        print(f"   Parameters: {len(entry_params)} fields")
        print(f"   Sample: confidence_threshold={entry_params['confidence_threshold_used']}")
        
        # Test crypto-specific parameters
        crypto_params = ml_collector.create_crypto_module_params(
            crypto_session="us_prime",
            volatility_score=0.72,
            momentum_score=0.85,
            volume_score=0.67,
            session_multiplier=1.2,
            analysis_weights={'momentum': 0.4, 'volatility': 0.3, 'volume': 0.3}
        )
        
        print("‚úÖ Crypto-specific parameters created successfully") 
        print(f"   Parameters: {len(crypto_params)} fields")
        print(f"   Sample: volatility_score={crypto_params['volatility_score']}")
        
        # Test complete ML trade data
        ml_trade_data = ml_collector.create_ml_trade_data(
            symbol="BTCUSD",
            side="BUY",
            quantity=0.1,
            price=50000,
            strategy="crypto_momentum",
            confidence=0.75,
            entry_parameters=entry_params,
            module_specific_params=crypto_params,
            profit_loss=750.50
        )
        
        print("‚úÖ Complete ML trade data created successfully")
        print(f"   Trade data fields: {len(ml_trade_data.to_dict())} total")
        print(f"   ML enhanced: {ml_trade_data.to_dict().get('ml_enhanced', False)}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ML data collection test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_ml_parameter_optimization():
    """Test ML parameter optimization engine"""
    print("\nüß™ TESTING ML PARAMETER OPTIMIZATION")
    print("="*50)
    
    try:
        from modular.ml_optimizer import MLParameterOptimizationEngine, ParameterOptimizer, BayesianOptimizer
        
        # Test Bayesian optimizer
        param_bounds = {
            'confidence_threshold': (0.4, 0.9),
            'position_multiplier': (0.5, 3.0)
        }
        
        bayesian_opt = BayesianOptimizer(param_bounds)
        
        # Test with mock historical data
        historical_data = [
            {'confidence_threshold': 0.6, 'position_multiplier': 1.5, 'profit_loss': 100.0, 'success': True},
            {'confidence_threshold': 0.7, 'position_multiplier': 2.0, 'profit_loss': 200.0, 'success': True},
            {'confidence_threshold': 0.5, 'position_multiplier': 1.0, 'profit_loss': -50.0, 'success': False},
            {'confidence_threshold': 0.8, 'position_multiplier': 2.5, 'profit_loss': 150.0, 'success': True},
            {'confidence_threshold': 0.65, 'position_multiplier': 1.8, 'profit_loss': 300.0, 'success': True}
        ]
        
        suggested_params = bayesian_opt.suggest_next_parameters(historical_data)
        
        print("‚úÖ Bayesian optimization working")
        print(f"   Suggested parameters: {suggested_params}")
        print(f"   Confidence threshold: {suggested_params.get('confidence_threshold', 'N/A'):.3f}")
        print(f"   Position multiplier: {suggested_params.get('position_multiplier', 'N/A'):.3f}")
        
        # Test parameter optimizer
        class MockFirebaseDB:
            def is_connected(self):
                return True
            def get_ml_optimization_data(self, module_name):
                return historical_data
            def get_module_parameters(self, module_name):
                return {'confidence_threshold': 0.6, 'position_multiplier': 1.5}
        
        mock_firebase = MockFirebaseDB()
        param_optimizer = ParameterOptimizer(mock_firebase)
        
        should_optimize = param_optimizer.should_optimize_parameters("test_module")
        print(f"‚úÖ Should optimize parameters: {should_optimize}")
        
        if should_optimize:
            optimization_results = param_optimizer.optimize_module_parameters("test_module")
            print(f"‚úÖ Parameter optimization results: {len(optimization_results)} parameters optimized")
            
            for result in optimization_results:
                print(f"   {result.parameter_type}: {result.old_value} -> {result.new_value} "
                      f"(expected improvement: {result.expected_improvement:.1%})")
        
        return True
        
    except Exception as e:
        print(f"‚ùå ML parameter optimization test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_entry_exit_linking():
    """Test critical entry-exit trade linking for profit learning"""
    print("\nüß™ TESTING ENTRY-EXIT TRADE LINKING")
    print("="*50)
    
    try:
        # Mock Firebase that tracks trade updates
        class MockFirebaseWithUpdates:
            def __init__(self):
                self.trades = {}
                self.updates = {}
                
            def save_trade(self, trade_data):
                trade_id = f"trade_{len(self.trades) + 1}"
                self.trades[trade_id] = trade_data.copy()
                print(f"   üìù Entry trade saved: {trade_id} - ${trade_data.get('profit_loss', 0):.2f}")
                return trade_id
                
            def update_trade_outcome(self, trade_id, outcome_data):
                if trade_id in self.trades:
                    self.trades[trade_id].update(outcome_data)
                    self.updates[trade_id] = outcome_data
                    print(f"   üí∞ Exit update applied: {trade_id} - Final P&L: ${outcome_data.get('profit_loss', 0):.2f}")
                    return True
                return False
                
            def is_connected(self):
                return True
        
        mock_firebase = MockFirebaseWithUpdates()
        
        # Simulate entry trade
        entry_trade_data = {
            'symbol': 'BTCUSD',
            'side': 'BUY',
            'quantity': 0.1,
            'price': 50000,
            'strategy': 'crypto_momentum',
            'confidence': 0.75,
            'profit_loss': 0.0,  # Entry starts with 0
            'timestamp': datetime.now().isoformat()
        }
        
        entry_trade_id = mock_firebase.save_trade(entry_trade_data)
        
        # Simulate exit with profit
        exit_outcome = {
            'profit_loss': 1250.75,  # Final profit
            'exit_reason': 'profit_target',
            'final_outcome': 'profitable',
            'hold_duration_hours': 4.5,
            'updated_at': datetime.now().isoformat()
        }
        
        update_success = mock_firebase.update_trade_outcome(entry_trade_id, exit_outcome)
        
        # Verify linking worked
        final_trade = mock_firebase.trades[entry_trade_id]
        
        print("‚úÖ Entry-exit linking verification:")
        print(f"   Entry P&L: ${entry_trade_data['profit_loss']:.2f}")
        print(f"   Final P&L: ${final_trade['profit_loss']:.2f}")
        print(f"   Exit reason: {final_trade.get('exit_reason', 'None')}")
        print(f"   Update success: {update_success}")
        
        # Critical check: profit was properly linked
        if (update_success and 
            final_trade['profit_loss'] == 1250.75 and
            final_trade.get('exit_reason') == 'profit_target'):
            print("‚úÖ CRITICAL: Entry-exit profit linking is working correctly")
            return True
        else:
            print("‚ùå CRITICAL: Entry-exit profit linking FAILED")
            return False
            
    except Exception as e:
        print(f"‚ùå Entry-exit linking test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_parameter_effectiveness_tracking():
    """Test parameter effectiveness tracking"""
    print("\nüß™ TESTING PARAMETER EFFECTIVENESS TRACKING")
    print("="*50)
    
    try:
        from modular.ml_data_helpers import ParameterEffectivenessTracker
        
        class MockFirebaseForParams:
            def __init__(self):
                self.param_data = []
                
            def save_parameter_effectiveness(self, param_data):
                self.param_data.append(param_data)
                param_id = f"param_{len(self.param_data)}"
                print(f"   üìä Parameter effectiveness saved: {param_data['parameter_type']}="
                      f"{param_data['parameter_value']} -> success={param_data['success']}")
                return param_id
                
            def get_parameter_effectiveness(self, module_name, parameter_type=None):
                return [p for p in self.param_data if p['module_name'] == module_name]
                
            def is_connected(self):
                return True
        
        mock_firebase = MockFirebaseForParams()
        tracker = ParameterEffectivenessTracker(mock_firebase, "crypto_module")
        
        # Record several parameter outcomes
        test_params = [
            ('confidence_threshold', 0.60, True, 150.0),
            ('confidence_threshold', 0.70, True, 200.0),
            ('confidence_threshold', 0.50, False, -75.0),
            ('position_multiplier', 1.5, True, 180.0),
            ('position_multiplier', 2.0, False, -120.0)
        ]
        
        for param_type, param_value, success, profit_loss in test_params:
            tracker.record_parameter_outcome(
                parameter_type=param_type,
                parameter_value=param_value,
                trade_outcome={'symbol': 'BTCUSD', 'strategy': 'crypto_momentum'},
                success=success,
                profit_loss=profit_loss
            )
        
        # Get effectiveness data
        effectiveness_data = tracker.get_parameter_effectiveness()
        
        print(f"‚úÖ Parameter effectiveness tracking working")
        print(f"   Total parameter records: {len(effectiveness_data)}")
        
        # Analyze effectiveness by parameter type
        confidence_records = [p for p in effectiveness_data if p['parameter_type'] == 'confidence_threshold']
        multiplier_records = [p for p in effectiveness_data if p['parameter_type'] == 'position_multiplier']
        
        print(f"   Confidence threshold records: {len(confidence_records)}")
        print(f"   Position multiplier records: {len(multiplier_records)}")
        
        if len(effectiveness_data) == len(test_params):
            print("‚úÖ All parameter outcomes tracked successfully")
            return True
        else:
            print(f"‚ùå Parameter tracking incomplete: {len(effectiveness_data)}/{len(test_params)}")
            return False
            
    except Exception as e:
        print(f"‚ùå Parameter effectiveness tracking test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_ml_integration_completeness():
    """Test that all modules have ML integration"""
    print("\nüß™ TESTING ML INTEGRATION COMPLETENESS")
    print("="*50)
    
    try:
        from modular.crypto_module import CryptoModule
        from modular.options_module import OptionsModule  
        from modular.stocks_module import StocksModule
        from modular.base_module import ModuleConfig
        
        modules_to_test = [
            ('CryptoModule', CryptoModule),
            ('OptionsModule', OptionsModule),
            ('StocksModule', StocksModule)
        ]
        
        ml_methods_required = [
            'ml_data_collector',
            'parameter_tracker',
            'ml_event_logger',
            'save_ml_enhanced_trade',
            'update_ml_trade_outcome',
            'record_parameter_effectiveness'
        ]
        
        results = {}
        
        for module_name, module_class in modules_to_test:
            print(f"\n   Testing {module_name}...")
            
            # Create mock dependencies
            class MockDependency:
                def is_connected(self):
                    return True
                    
            mock_config = ModuleConfig(module_name.lower(), enabled=True)
            mock_firebase = MockDependency()
            mock_risk_manager = MockDependency()
            mock_order_executor = MockDependency()
            mock_api_client = MockDependency()
            
            try:
                # Initialize module with required dependencies
                if module_name == 'CryptoModule':
                    module_instance = module_class(mock_config, mock_firebase, mock_risk_manager, 
                                                 mock_order_executor, mock_api_client)
                else:
                    module_instance = module_class(mock_config, mock_firebase, mock_risk_manager, 
                                                 mock_order_executor, mock_api_client)
                
                # Check for ML methods
                missing_methods = []
                for method in ml_methods_required:
                    if not hasattr(module_instance, method):
                        missing_methods.append(method)
                
                if not missing_methods:
                    print(f"   ‚úÖ {module_name}: All ML methods present")
                    results[module_name] = True
                else:
                    print(f"   ‚ùå {module_name}: Missing ML methods: {missing_methods}")
                    results[module_name] = False
                    
            except Exception as e:
                print(f"   ‚ùå {module_name}: Failed to initialize - {e}")
                results[module_name] = False
        
        # Summary
        successful_modules = sum(results.values())
        total_modules = len(results)
        
        print(f"\nüìä ML Integration Summary:")
        print(f"   Modules with complete ML integration: {successful_modules}/{total_modules}")
        
        if successful_modules == total_modules:
            print("‚úÖ ALL MODULES HAVE COMPLETE ML INTEGRATION")
            return True
        else:
            print("‚ùå SOME MODULES MISSING ML INTEGRATION")
            return False
            
    except Exception as e:
        print(f"‚ùå ML integration completeness test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """Run comprehensive ML audit"""
    print("üß† ML SYSTEM COMPREHENSIVE AUDIT")
    print("="*60)
    print(f"Timestamp: {datetime.now().isoformat()}")
    print("="*60)
    
    tests = [
        ("ML Data Collection", test_ml_data_collection),
        ("ML Parameter Optimization", test_ml_parameter_optimization), 
        ("Entry-Exit Trade Linking", test_entry_exit_linking),
        ("Parameter Effectiveness Tracking", test_parameter_effectiveness_tracking),
        ("ML Integration Completeness", test_ml_integration_completeness)
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"‚ùå {test_name} failed with exception: {e}")
            results[test_name] = False
    
    # Final report
    print("\n" + "="*60)
    print("üéØ ML AUDIT FINAL REPORT")
    print("="*60)
    
    passed_tests = sum(results.values())
    total_tests = len(results)
    
    for test_name, passed in results.items():
        status = "‚úÖ PASS" if passed else "‚ùå FAIL"
        print(f"{status} {test_name}")
    
    print(f"\nüìä Overall Score: {passed_tests}/{total_tests} tests passed")
    
    if passed_tests == total_tests:
        print("\nüéâ ML SYSTEM IS FULLY OPERATIONAL AND ROBUST!")
        print("‚úÖ ML data collection: ACTIVE")
        print("‚úÖ Parameter optimization: ACTIVE") 
        print("‚úÖ Entry-exit linking: ACTIVE")
        print("‚úÖ Parameter tracking: ACTIVE")
        print("‚úÖ All modules integrated: ACTIVE")
        return True
    else:
        print(f"\n‚ö†Ô∏è ML SYSTEM HAS {total_tests - passed_tests} ISSUES THAT NEED ATTENTION")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)